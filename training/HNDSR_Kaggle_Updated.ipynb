{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892e5d90",
   "metadata": {},
   "source": [
    "# HNDSR: A Hybrid Neural Operator–Diffusion Model for Continuous-Scale Satellite Image Super-Resolution\n",
    "\n",
    "**Kaggle Training Notebook — Extended Epochs & Optimised Pipeline**\n",
    "\n",
    "> **Authors:** Adil Khan, Rakshit Modanwal, Harsh Vardhan, Piyush Jain, Yash Vikram  \n",
    "> **Institution:** Indian Institute of Information Technology, Nagpur  \n",
    "> **Dataset:** [4× Satellite Image Super-Resolution](https://www.kaggle.com/datasets/cristobaltudela/4x-satellite-image-super-resolution)\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "HNDSR is a hybrid super-resolution framework that fuses **Neural Operators** (for continuous-scale awareness) with **Latent Diffusion Models** (for high-fidelity texture generation). The architecture trains in three sequential stages:\n",
    "\n",
    "| Stage | Component | Purpose | Loss |\n",
    "|------:|-----------|---------|------|\n",
    "| 1 | **Latent Autoencoder** | Learn a compact latent space from HR images | L1 (reconstruction) |\n",
    "| 2 | **Fourier Neural Operator** | Map LR → HR latents with scale-invariance | MSE (latent matching) |\n",
    "| 3 | **Diffusion UNet** | Refine high-frequency details via iterative denoising | MSE (noise prediction) |\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{AE}} + \\lambda_{\\text{NO}} \\cdot \\mathcal{L}_{\\text{NO}} + \\lambda_{\\text{diff}} \\cdot \\mathcal{L}_{\\text{diff}}$$\n",
    "\n",
    "### Key Results (4× upscaling)\n",
    "\n",
    "| Method | PSNR ↑ | SSIM ↑ | LPIPS ↓ |\n",
    "|--------|--------|--------|---------|\n",
    "| Bicubic | 24.53 | 0.71 | 0.35 |\n",
    "| EDSR | 26.81 | 0.79 | 0.28 |\n",
    "| ESRGAN | 27.14 | 0.81 | 0.24 |\n",
    "| E²DiffSR | 28.72 | 0.85 | 0.18 |\n",
    "| **HNDSR (Ours)** | **29.40** | **0.87** | **0.16** |\n",
    "\n",
    "### Notebook Features\n",
    "- Saves all checkpoints to `/kaggle/working/` (persists as notebook output)\n",
    "- Configurable epoch counts per stage — defaults are **2× the original**\n",
    "- **Gradient clipping**, **warmup + cosine annealing**, **early stopping**\n",
    "- **EMA (Exponential Moving Average)** on diffusion weights\n",
    "- Extended data augmentation (horizontal + vertical flip, 90° rotations)\n",
    "- Resume support if kernel restarts — just set `START_FROM_STAGE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89749401",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "\n",
    "Install all dependencies and verify GPU availability. Kaggle provides free **P100 (16 GB)** or **T4 (16 GB)** GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5444ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision lpips timm einops scikit-image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import lpips\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False   # deterministic but slightly slower\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device       : {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU          : {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU Memory   : {gpu_mem:.2f} GB\")\n",
    "    print(f\"CUDA version : {torch.version.cuda}\")\n",
    "print(f\"PyTorch      : {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccf0af",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Training Configuration\n",
    "\n",
    "All hyper-parameters in one place. Edit the epoch counts below to train longer.\n",
    "\n",
    "> **Tip:** Kaggle gives ~12 h of GPU per session. With `BATCH_SIZE=2` the full 130-epoch\n",
    "> pipeline fits within that budget on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e3dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "#  TRAINING CONFIGURATION  —  EDIT THESE VALUES\n",
    "# =================================================================\n",
    "\n",
    "# Epoch counts per stage  (original → new default)\n",
    "STAGE1_EPOCHS = 40       # Autoencoder       (was 20)\n",
    "STAGE2_EPOCHS = 30       # Neural Operator   (was 15)\n",
    "STAGE3_EPOCHS = 60       # Diffusion Model   (was 30)\n",
    "\n",
    "# Optimiser & data\n",
    "BATCH_SIZE    = 2        # reduce to 1 if OOM on 16 GB GPU\n",
    "PATCH_SIZE    = 64       # HR crop size (must be divisible by 8)\n",
    "LEARNING_RATE = 1e-4     # peak LR (after warmup)\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "NUM_WORKERS   = 2        # Kaggle max = 2\n",
    "\n",
    "# Gradient clipping  (stabilises early training)\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Early stopping  (patience = epochs with no val improvement)\n",
    "EARLY_STOP_PATIENCE = 10  # set to 999 to disable\n",
    "\n",
    "# Warmup  (linear LR warmup before cosine decay)\n",
    "WARMUP_EPOCHS = 3\n",
    "\n",
    "# EMA  (Exponential Moving Average for diffusion weights)\n",
    "EMA_DECAY = 0.999\n",
    "\n",
    "# Resume from a specific stage  (1 = train all stages from scratch)\n",
    "START_FROM_STAGE = 1\n",
    "\n",
    "# Auto-resume: automatically pick up from the last saved epoch\n",
    "# within a stage. Set to True when re-running after a session timeout.\n",
    "AUTO_RESUME = True\n",
    "\n",
    "# =================================================================\n",
    "#  PATHS  —  auto-configured for Kaggle\n",
    "# =================================================================\n",
    "SAVE_DIR             = '/kaggle/working'\n",
    "AUTOENCODER_PATH     = os.path.join(SAVE_DIR, 'autoencoder_best.pth')\n",
    "NEURAL_OPERATOR_PATH = os.path.join(SAVE_DIR, 'neural_operator_best.pth')\n",
    "DIFFUSION_PATH       = os.path.join(SAVE_DIR, 'diffusion_best.pth')\n",
    "COMPLETE_MODEL_PATH  = os.path.join(SAVE_DIR, 'hndsr_complete.pth')\n",
    "EVAL_RESULTS_DIR     = os.path.join(SAVE_DIR, 'evaluation_results')\n",
    "\n",
    "# Epoch-level resume checkpoints (saved every epoch so you never\n",
    "# lose more than 1 epoch of work if the session is interrupted)\n",
    "STAGE1_RESUME_PATH   = os.path.join(SAVE_DIR, 'stage1_resume.pth')\n",
    "STAGE2_RESUME_PATH   = os.path.join(SAVE_DIR, 'stage2_resume.pth')\n",
    "STAGE3_RESUME_PATH   = os.path.join(SAVE_DIR, 'stage3_resume.pth')\n",
    "\n",
    "# =================================================================\n",
    "print(f\"Stage 1 epochs : {STAGE1_EPOCHS}\")\n",
    "print(f\"Stage 2 epochs : {STAGE2_EPOCHS}\")\n",
    "print(f\"Stage 3 epochs : {STAGE3_EPOCHS}\")\n",
    "print(f\"Total epochs   : {STAGE1_EPOCHS + STAGE2_EPOCHS + STAGE3_EPOCHS}\")\n",
    "print(f\"Batch size     : {BATCH_SIZE}\")\n",
    "print(f\"Grad clip norm : {MAX_GRAD_NORM}\")\n",
    "print(f\"Early-stop     : {EARLY_STOP_PATIENCE} epochs patience\")\n",
    "print(f\"EMA decay      : {EMA_DECAY}\")\n",
    "print(f\"Auto-resume    : {AUTO_RESUME}\")\n",
    "print(f\"Save directory : {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d2fc6",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Dataset Detection\n",
    "\n",
    "**Dataset:** [cristobaltudela/4x-satellite-image-super-resolution](https://www.kaggle.com/datasets/cristobaltudela/4x-satellite-image-super-resolution)\n",
    "\n",
    "Before running this notebook, **add the dataset** to your Kaggle notebook:\n",
    "1. Click **+ Add Data** (right panel) → search for `4x-satellite-image-super-resolution` → Add\n",
    "2. It will appear under `/kaggle/input/`\n",
    "\n",
    "The cell below auto-detects the HR and LR sub-directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff33f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dataset():\n",
    "    \"\"\"Auto-detect HR / LR directories under /kaggle/input/\"\"\"\n",
    "    kaggle_input = Path('/kaggle/input')\n",
    "    if not kaggle_input.exists():\n",
    "        print(\"Not running on Kaggle — set HR_DIR / LR_DIR manually below.\")\n",
    "        return None, None\n",
    "\n",
    "    datasets = sorted([d for d in kaggle_input.iterdir() if d.is_dir()])\n",
    "    print(f\"Datasets found: {[d.name for d in datasets]}\")\n",
    "\n",
    "    # Try to find a dataset with relevant keywords\n",
    "    sr_datasets = [d for d in datasets\n",
    "                   if any(k in d.name.lower() for k in ('super', 'resolution', 'satellite', '4x'))]\n",
    "    dataset_path = sr_datasets[0] if sr_datasets else (datasets[0] if datasets else None)\n",
    "\n",
    "    if dataset_path is None:\n",
    "        print(\"No dataset folder found — please add the dataset first.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Using: {dataset_path}\")\n",
    "\n",
    "    hr_dir = lr_dir = None\n",
    "    img_exts = {'.png', '.jpg', '.jpeg', '.tif', '.tiff'}\n",
    "\n",
    "    for item in sorted(dataset_path.rglob('*')):\n",
    "        if not item.is_dir():\n",
    "            continue\n",
    "        name_lower = item.name.lower()\n",
    "        imgs = [f for f in item.iterdir() if f.suffix.lower() in img_exts]\n",
    "        if not imgs:\n",
    "            continue\n",
    "        if ('hr' in name_lower or 'high' in name_lower) and hr_dir is None:\n",
    "            hr_dir = str(item)\n",
    "            print(f\"  HR directory : {item}  ({len(imgs)} images)\")\n",
    "        elif ('lr' in name_lower or 'low' in name_lower) and lr_dir is None:\n",
    "            lr_dir = str(item)\n",
    "            print(f\"  LR directory : {item}  ({len(imgs)} images)\")\n",
    "\n",
    "    return hr_dir, lr_dir\n",
    "\n",
    "HR_DIR, LR_DIR = find_dataset()\n",
    "\n",
    "# ── MANUAL OVERRIDE (uncomment if auto-detect fails) ──────────────\n",
    "# HR_DIR = '/kaggle/input/4x-satellite-image-super-resolution/HR_0.5m'\n",
    "# LR_DIR = '/kaggle/input/4x-satellite-image-super-resolution/LR_2m'\n",
    "# ───────────────────────────────────────────────────────────────────\n",
    "\n",
    "assert HR_DIR and LR_DIR, (\n",
    "    \"Dataset paths not found.  Please:\\n\"\n",
    "    \"  1. Add 'cristobaltudela/4x-satellite-image-super-resolution' via + Add Data\\n\"\n",
    "    \"  2. Or uncomment the MANUAL OVERRIDE lines above and set the correct paths.\"\n",
    ")\n",
    "print(f\"\\nHR_DIR = {HR_DIR}\")\n",
    "print(f\"LR_DIR = {LR_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06483f7c",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Dataset & Augmentation\n",
    "\n",
    "The dataset contains paired satellite images at two GSD (Ground Sampling Distance) levels:\n",
    "\n",
    "| Folder | Resolution | GSD |\n",
    "|--------|-----------|-----|\n",
    "| `HR_0.5m/` | High resolution | 0.5 m/pixel |\n",
    "| `LR_2m/`   | Low resolution  | 2.0 m/pixel |\n",
    "\n",
    "This gives a natural **4× scale factor**.\n",
    "\n",
    "**Augmentations** applied during training:\n",
    "- Random crop (64×64 HR patch ↔ 16×16 LR patch)\n",
    "- Horizontal flip (p = 0.5)\n",
    "- Vertical flip (p = 0.5)\n",
    "- 90° rotation (p = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba374813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    \"\"\"Paired satellite SR dataset with augmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, hr_dir, lr_dir, patch_size=64, training=True):\n",
    "        self.hr_dir    = Path(hr_dir)\n",
    "        self.lr_dir    = Path(lr_dir)\n",
    "        self.patch_size = patch_size\n",
    "        self.training   = training\n",
    "\n",
    "        # Gather images\n",
    "        exts = ['*.png', '*.jpg', '*.jpeg', '*.tif', '*.tiff',\n",
    "                '*.PNG', '*.JPG', '*.JPEG', '*.TIF', '*.TIFF']\n",
    "        self.hr_images, self.lr_images = [], []\n",
    "        for ext in exts:\n",
    "            self.hr_images.extend(self.hr_dir.glob(ext))\n",
    "            self.lr_images.extend(self.lr_dir.glob(ext))\n",
    "        self.hr_images = sorted(self.hr_images)\n",
    "        self.lr_images = sorted(self.lr_images)\n",
    "\n",
    "        if not self.hr_images or not self.lr_images:\n",
    "            raise ValueError(f\"No images found!  HR={hr_dir}  LR={lr_dir}\")\n",
    "\n",
    "        # Match by filename stem (fall back to positional pairing)\n",
    "        hr_map = {img.stem: img for img in self.hr_images}\n",
    "        lr_map = {img.stem: img for img in self.lr_images}\n",
    "        common = sorted(set(hr_map) & set(lr_map))\n",
    "\n",
    "        if common:\n",
    "            self.hr_images = [hr_map[n] for n in common]\n",
    "            self.lr_images = [lr_map[n] for n in common]\n",
    "        else:\n",
    "            n = min(len(self.hr_images), len(self.lr_images))\n",
    "            self.hr_images = self.hr_images[:n]\n",
    "            self.lr_images = self.lr_images[:n]\n",
    "\n",
    "        print(f\"{'Train' if training else 'Eval'} dataset: {len(self.hr_images)} pairs\")\n",
    "\n",
    "        self.to_tensor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3),  # → [-1, 1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "\n",
    "    def _augment(self, hr_pil, lr_pil):\n",
    "        \"\"\"Apply paired random augmentation.\"\"\"\n",
    "        # Horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            hr_pil = hr_pil.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            lr_pil = lr_pil.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        # Vertical flip\n",
    "        if random.random() > 0.5:\n",
    "            hr_pil = hr_pil.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            lr_pil = lr_pil.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        # 90° rotation\n",
    "        if random.random() > 0.5:\n",
    "            hr_pil = hr_pil.transpose(Image.ROTATE_90)\n",
    "            lr_pil = lr_pil.transpose(Image.ROTATE_90)\n",
    "        return hr_pil, lr_pil\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hr_img = Image.open(self.hr_images[idx]).convert('RGB')\n",
    "        lr_img = Image.open(self.lr_images[idx]).convert('RGB')\n",
    "\n",
    "        if self.training:\n",
    "            hr_w, hr_h = hr_img.size\n",
    "            lr_w, lr_h = lr_img.size\n",
    "            scale    = max(hr_w // lr_w, 1)\n",
    "            lr_crop  = self.patch_size // scale\n",
    "\n",
    "            # Random crop\n",
    "            if lr_w > lr_crop and lr_h > lr_crop:\n",
    "                x = random.randint(0, lr_w - lr_crop)\n",
    "                y = random.randint(0, lr_h - lr_crop)\n",
    "                lr_img = lr_img.crop((x, y, x + lr_crop, y + lr_crop))\n",
    "                hr_img = hr_img.crop((x*scale, y*scale,\n",
    "                                      (x+lr_crop)*scale, (y+lr_crop)*scale))\n",
    "\n",
    "            hr_img, lr_img = self._augment(hr_img, lr_img)\n",
    "        else:\n",
    "            hr_img = transforms.CenterCrop(self.patch_size)(hr_img)\n",
    "            lr_img = transforms.CenterCrop(self.patch_size // 4)(lr_img)\n",
    "\n",
    "        return {\n",
    "            'lr':    self.to_tensor(lr_img),\n",
    "            'hr':    self.to_tensor(hr_img),\n",
    "            'scale': 4,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a369bcc",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Architecture\n",
    "\n",
    "### 5.1 Stage 1 — Latent Autoencoder\n",
    "\n",
    "The autoencoder learns a **compressed latent representation** of HR satellite images.\n",
    "It uses an encoder-decoder structure with **residual blocks** and a spatial\n",
    "down-sampling ratio of 8× — mapping a 64×64 HR patch to an 8×8 latent tensor\n",
    "with 128 channels.\n",
    "\n",
    "$$z = E_\\theta(x_\\text{HR}), \\quad \\hat{x} = D_\\theta(z), \\quad \\mathcal{L}_\\text{AE} = \\| \\hat{x} - x_\\text{HR} \\|_1$$\n",
    "\n",
    "### 5.2 Stage 2 — Fourier Neural Operator (FNO)\n",
    "\n",
    "Neural Operators learn mappings between **function spaces** rather than finite\n",
    "vectors, enabling continuous-scale super-resolution. The core primitive is the\n",
    "**Spectral Convolution** layer:\n",
    "\n",
    "$$(\\mathcal{K}v)(x) = \\mathcal{F}^{-1}\\!\\left( R \\cdot \\mathcal{F}(v) \\right)(x)$$\n",
    "\n",
    "where $\\mathcal{F}$ is the 2-D FFT and $R$ is a learnable weight tensor applied\n",
    "to the first $k$ Fourier modes. Four such layers are stacked, each followed by\n",
    "a pointwise 1×1 convolution (local bypass) and GELU activation.\n",
    "\n",
    "A **scale map** (constant channel encoding the target scale factor) is\n",
    "concatenated to the input so the operator learns scale-aware features.\n",
    "\n",
    "### 5.3 Implicit Amplification\n",
    "\n",
    "A small MLP predicts per-channel gains $\\gamma_c \\in [0, 1]$ from the scale\n",
    "factor and modulates the latent:\n",
    "\n",
    "$$z' = z \\odot (1 + \\gamma)$$\n",
    "\n",
    "This amplifies high-frequency components proportionally to the upscaling ratio.\n",
    "\n",
    "### 5.4 Stage 3 — Latent Diffusion UNet\n",
    "\n",
    "A lightweight UNet iteratively denoises a Gaussian noise sample $z_T$ into the\n",
    "target HR latent $z_0$, conditioned on the Neural Operator prior via\n",
    "**cross-attention**:\n",
    "\n",
    "$$z_{t-1} = \\text{DDIM}(z_t, \\epsilon_\\theta(z_t, t, c)), \\quad c = \\text{pool}(z'_\\text{NO})$$\n",
    "\n",
    "The UNet uses **sinusoidal positional time embeddings**, GroupNorm, and SiLU\n",
    "activations — following the Stable Diffusion convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  Stage 1 — Residual Block & Latent Autoencoder\n",
    "# =====================================================================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Two-conv residual block used inside the autoencoder.\"\"\"\n",
    "    def __init__(self, channels, use_bn=False):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(channels, channels, 3, padding=1),\n",
    "                  nn.ReLU(inplace=True),\n",
    "                  nn.Conv2d(channels, channels, 3, padding=1)]\n",
    "        if use_bn:\n",
    "            layers.insert(1, nn.BatchNorm2d(channels))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class LatentAutoencoder(nn.Module):\n",
    "    \"\"\"Encoder-Decoder that maps 3-ch images to a compact latent z and back.\n",
    "\n",
    "    Default: 64→128 channels, 3 down-samples (8× spatial reduction).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, latent_dim=64, num_res_blocks=4, downsample_ratio=8):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.downsample_ratio = downsample_ratio\n",
    "        num_downs = int(math.log2(downsample_ratio))\n",
    "\n",
    "        # Encoder\n",
    "        enc = [nn.Conv2d(in_channels, latent_dim, 3, padding=1)]\n",
    "        ch = latent_dim\n",
    "        for _ in range(num_downs):\n",
    "            out_ch = min(ch * 2, 128)\n",
    "            enc += [nn.Conv2d(ch, out_ch, 4, stride=2, padding=1), nn.ReLU(True)]\n",
    "            ch = out_ch\n",
    "        for _ in range(num_res_blocks):\n",
    "            enc.append(ResidualBlock(ch))\n",
    "        self.encoder = nn.Sequential(*enc)\n",
    "\n",
    "        # Decoder\n",
    "        dec = [ResidualBlock(ch) for _ in range(num_res_blocks)]\n",
    "        for _ in range(num_downs):\n",
    "            out_ch = ch // 2\n",
    "            dec += [nn.ConvTranspose2d(ch, out_ch, 4, stride=2, padding=1), nn.ReLU(True)]\n",
    "            ch = out_ch\n",
    "        dec += [nn.Conv2d(ch, in_channels, 3, padding=1), nn.Tanh()]\n",
    "        self.decoder = nn.Sequential(*dec)\n",
    "\n",
    "    def encode(self, x):  return self.encoder(x)\n",
    "    def decode(self, z):  return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z), z\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#  Stage 2 — Fourier Neural Operator\n",
    "# =====================================================================\n",
    "\n",
    "class SpectralConv2d(nn.Module):\n",
    "    \"\"\"2-D spectral convolution — applies learnable weights in Fourier space.\n",
    "\n",
    "    Forces float32 FFT to avoid cuFFT half-precision power-of-2 requirement.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, modes1, modes2):\n",
    "        super().__init__()\n",
    "        self.in_channels  = in_ch\n",
    "        self.out_channels = out_ch\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        scale = 1.0 / (in_ch * out_ch)\n",
    "        self.weights1 = nn.Parameter(scale * torch.rand(in_ch, out_ch, modes1, modes2, 2))\n",
    "        self.weights2 = nn.Parameter(scale * torch.rand(in_ch, out_ch, modes1, modes2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_dtype = x.dtype\n",
    "        x = x.float()\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        out_ft = torch.zeros(x.shape[0], self.out_channels, x.size(-2),\n",
    "                             x.size(-1)//2+1, dtype=torch.cfloat, device=x.device)\n",
    "\n",
    "        m1 = min(self.modes1, x.size(-2))\n",
    "        m2 = min(self.modes2, x.size(-1)//2+1)\n",
    "\n",
    "        if m1 > 0 and m2 > 0:\n",
    "            w1 = torch.view_as_complex(self.weights1[:, :, :m1, :m2])\n",
    "            w2 = torch.view_as_complex(self.weights2[:, :, :m1, :m2])\n",
    "            out_ft[:, :, :m1,  :m2] = torch.einsum('bixy,ioxy->boxy', x_ft[:, :, :m1,  :m2], w1)\n",
    "            out_ft[:, :, -m1:, :m2] = torch.einsum('bixy,ioxy->boxy', x_ft[:, :, -m1:, :m2], w2)\n",
    "\n",
    "        x_out = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x_out.to(x_dtype) if x_dtype != torch.float32 else x_out\n",
    "\n",
    "\n",
    "class NeuralOperator(nn.Module):\n",
    "    \"\"\"FNO with 4 spectral layers and scale-map conditioning.\"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=128, modes=8, width=32):\n",
    "        super().__init__()\n",
    "        self.fc0  = nn.Conv2d(in_channels + 1, width, 1)\n",
    "        self.convs = nn.ModuleList([SpectralConv2d(width, width, modes, modes) for _ in range(4)])\n",
    "        self.ws    = nn.ModuleList([nn.Conv2d(width, width, 1) for _ in range(4)])\n",
    "        self.fc1  = nn.Conv2d(width, 64, 1)\n",
    "        self.fc2  = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def forward(self, x, scale_factor):\n",
    "        b, c, h, w = x.shape\n",
    "        scale_map = torch.ones(b, 1, h, w, device=x.device) * (scale_factor / 4.0)\n",
    "        x = self.fc0(torch.cat([x, scale_map], 1))\n",
    "        for conv, w_conv in zip(self.convs, self.ws):\n",
    "            x = F.gelu(conv(x) + w_conv(x))\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#  Implicit Amplification MLP\n",
    "# =====================================================================\n",
    "\n",
    "class ImplicitAmplification(nn.Module):\n",
    "    \"\"\"Scale-conditioned channel gain predictor.\"\"\"\n",
    "    def __init__(self, latent_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim), nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim, latent_dim), nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, latent, scale_factor):\n",
    "        b = latent.shape[0]\n",
    "        s = torch.full((b, 1), float(scale_factor) if isinstance(scale_factor, (int, float))\n",
    "                       else scale_factor.item(), device=latent.device, dtype=torch.float32)\n",
    "        return latent * (1 + self.mlp(s).view(b, -1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  Stage 3 — Diffusion UNet Components\n",
    "# =====================================================================\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    \"\"\"Maps integer timestep t → sinusoidal embedding (same as Transformer PE).\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        half = self.dim // 2\n",
    "        emb = math.log(10000) / (half - 1)\n",
    "        emb = torch.exp(torch.arange(half, device=time.device) * -emb)\n",
    "        emb = time[:, None] * emb[None, :]\n",
    "        return torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Spatial self-attention (used optionally in the UNet).\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(min(8, channels), channels)\n",
    "        self.qkv  = nn.Conv2d(channels, channels * 3, 1)\n",
    "        self.proj  = nn.Conv2d(channels, channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        res = x\n",
    "        x = self.norm(x)\n",
    "        q, k, v = self.qkv(x).chunk(3, 1)\n",
    "        q = q.view(b, c, -1).transpose(1, 2)\n",
    "        k = k.view(b, c, -1).transpose(1, 2)\n",
    "        v = v.view(b, c, -1).transpose(1, 2)\n",
    "        attn = torch.softmax(torch.bmm(q, k.transpose(1, 2)) * (c ** -0.5), dim=-1)\n",
    "        out = torch.bmm(attn, v).transpose(1, 2).view(b, c, h, w)\n",
    "        return self.proj(out) + res\n",
    "\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"Cross-attention: UNet features attend to Neural-Operator context vector.\"\"\"\n",
    "    def __init__(self, channels, context_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(min(8, channels), channels)\n",
    "        self.q  = nn.Conv2d(channels, channels, 1)\n",
    "        self.kv = nn.Linear(context_dim, channels * 2)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        b, c, h, w = x.shape\n",
    "        res = x\n",
    "        x = self.norm(x)\n",
    "        q = self.q(x).view(b, c, -1).transpose(1, 2)\n",
    "        kv = self.kv(context)\n",
    "        k, v = kv.chunk(2, 1)\n",
    "        k, v = k.unsqueeze(1), v.unsqueeze(1)\n",
    "        attn = torch.softmax(torch.bmm(q, k.transpose(1, 2)) * (c ** -0.5), dim=-1)\n",
    "        out = torch.bmm(attn, v).transpose(1, 2).view(b, c, h, w)\n",
    "        return self.proj(out) + res\n",
    "\n",
    "\n",
    "class ResidualBlockWithTime(nn.Module):\n",
    "    \"\"\"ResBlock conditioned on a timestep embedding (added after first conv).\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, t_dim):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(min(8, in_ch), in_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.time_emb = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, out_ch))\n",
    "        self.norm2 = nn.GroupNorm(min(8, out_ch), out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.shortcut = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.conv1(F.silu(self.norm1(x)))\n",
    "        h = h + self.time_emb(t_emb)[:, :, None, None]\n",
    "        h = self.conv2(F.silu(self.norm2(h)))\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "\n",
    "class DiffusionUNet(nn.Module):\n",
    "    \"\"\"Simplified UNet for latent-space denoising.\n",
    "\n",
    "    Architecture:  input_proj → Down → Mid (cross-attn) → Up → output\n",
    "    Skip connection from input_proj to Up via concatenation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=128, model_channels=64,\n",
    "                 out_channels=128, context_dim=128):\n",
    "        super().__init__()\n",
    "        t_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(model_channels),\n",
    "            nn.Linear(model_channels, t_dim), nn.SiLU(),\n",
    "            nn.Linear(t_dim, t_dim),\n",
    "        )\n",
    "\n",
    "        self.input_proj = nn.Conv2d(in_channels, model_channels, 3, padding=1)\n",
    "\n",
    "        # ---- Down ----\n",
    "        self.down1 = ResidualBlockWithTime(model_channels, model_channels * 2, t_dim)\n",
    "        self.down2 = nn.Conv2d(model_channels * 2, model_channels * 2, 3, stride=2, padding=1)\n",
    "\n",
    "        # ---- Mid ----\n",
    "        self.mid1     = ResidualBlockWithTime(model_channels * 2, model_channels * 2, t_dim)\n",
    "        self.mid_attn = CrossAttentionBlock(model_channels * 2, context_dim)\n",
    "        self.mid2     = ResidualBlockWithTime(model_channels * 2, model_channels * 2, t_dim)\n",
    "\n",
    "        # ---- Up ----\n",
    "        self.up1 = nn.ConvTranspose2d(model_channels * 2, model_channels * 2,\n",
    "                                      4, stride=2, padding=1)\n",
    "        # After concat with skip:  model_channels*2 + model_channels = model_channels*3\n",
    "        self.up2 = ResidualBlockWithTime(model_channels * 3, model_channels, t_dim)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(8, model_channels), nn.SiLU(),\n",
    "            nn.Conv2d(model_channels, out_channels, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, context):\n",
    "        t_emb = self.time_embed(t)\n",
    "        h  = self.input_proj(x)    # skip source\n",
    "        h0 = h\n",
    "        h  = self.down2(self.down1(h, t_emb))\n",
    "        h  = self.mid2(self.mid_attn(self.mid1(h, t_emb), context), t_emb)\n",
    "        h  = self.up2(torch.cat([self.up1(h), h0], dim=1), t_emb)\n",
    "        return self.out(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f56f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  DDPM Noise Scheduler\n",
    "# =====================================================================\n",
    "\n",
    "class DDPMScheduler:\n",
    "    \"\"\"Linear-beta DDPM scheduler with DDIM sampling.\"\"\"\n",
    "    def __init__(self, num_timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, 0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "\n",
    "    def add_noise(self, x0, noise, t):\n",
    "        \"\"\"q(x_t | x_0) forward diffusion.\"\"\"\n",
    "        t_cpu = t.cpu()\n",
    "        a = self.sqrt_alphas_cumprod[t_cpu].to(x0.device)\n",
    "        b = self.sqrt_one_minus_alphas_cumprod[t_cpu].to(x0.device)\n",
    "        while a.dim() < x0.dim(): a = a.unsqueeze(-1)\n",
    "        while b.dim() < x0.dim(): b = b.unsqueeze(-1)\n",
    "        return a * x0 + b * noise\n",
    "\n",
    "    def ddim_sample(self, eps_pred, t, x_t):\n",
    "        \"\"\"Deterministic DDIM reverse step.\"\"\"\n",
    "        t_val = t.item() if isinstance(t, torch.Tensor) and t.numel() == 1 else t\n",
    "        a_t    = self.alphas_cumprod[t_val].to(x_t.device)\n",
    "        a_prev = (self.alphas_cumprod_prev[t_val].to(x_t.device)\n",
    "                  if t_val > 0 else torch.tensor(1.0, device=x_t.device))\n",
    "        x0_pred = (x_t - torch.sqrt(1 - a_t) * eps_pred) / torch.sqrt(a_t)\n",
    "        return torch.sqrt(a_prev) * x0_pred + torch.sqrt(1 - a_prev) * eps_pred\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#  EMA (Exponential Moving Average) Helper\n",
    "# =====================================================================\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"Maintains an exponential moving average of model parameters.\"\"\"\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {k: v.clone().detach() for k, v in model.state_dict().items()}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for k, v in model.state_dict().items():\n",
    "            if self.shadow[k].device != v.device:\n",
    "                self.shadow[k] = self.shadow[k].to(v.device)\n",
    "            self.shadow[k].mul_(self.decay).add_(v, alpha=1 - self.decay)\n",
    "\n",
    "    def apply(self, model):\n",
    "        model.load_state_dict(self.shadow)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.shadow\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "#  Complete HNDSR Model\n",
    "# =====================================================================\n",
    "\n",
    "class HNDSR(nn.Module):\n",
    "    \"\"\"Hybrid Neural Operator–Diffusion Super-Resolution.\"\"\"\n",
    "    def __init__(self, ae_latent_dim=128, ae_downsample_ratio=8,\n",
    "                 no_width=32, no_modes=8, diffusion_channels=64,\n",
    "                 num_timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.ae_downsample_ratio = ae_downsample_ratio\n",
    "        self.autoencoder     = LatentAutoencoder(3, ae_latent_dim, 4, ae_downsample_ratio)\n",
    "        self.neural_operator = NeuralOperator(3, ae_latent_dim, no_modes, no_width)\n",
    "        self.implicit_amp    = ImplicitAmplification(ae_latent_dim, 256)\n",
    "        self.diffusion_unet  = DiffusionUNet(ae_latent_dim, diffusion_channels,\n",
    "                                             ae_latent_dim, ae_latent_dim)\n",
    "        self.scheduler       = DDPMScheduler(num_timesteps)\n",
    "\n",
    "    # ---- Convenience wrappers ----\n",
    "    def encode_hr(self, hr):\n",
    "        _, z = self.autoencoder(hr)\n",
    "        return z\n",
    "\n",
    "    def decode_latent(self, z):\n",
    "        return self.autoencoder.decode(z)\n",
    "\n",
    "    def get_no_prior(self, lr, scale):\n",
    "        up   = F.interpolate(lr, scale_factor=scale, mode='bicubic', align_corners=False)\n",
    "        feat = self.neural_operator(up, scale)\n",
    "        s    = up.shape[-1] // self.ae_downsample_ratio\n",
    "        return F.interpolate(feat, size=(s, s), mode='bilinear', align_corners=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def super_resolve(self, lr, scale_factor=4, num_inference_steps=50):\n",
    "        \"\"\"Full inference: LR → SR image via diffusion sampling.\"\"\"\n",
    "        b = lr.shape[0]\n",
    "        no_prior = self.implicit_amp(self.get_no_prior(lr, scale_factor), scale_factor)\n",
    "        context  = F.adaptive_avg_pool2d(no_prior, 1).view(b, -1)\n",
    "        z_t = torch.randn(no_prior.shape, device=lr.device)\n",
    "\n",
    "        timesteps = torch.linspace(self.scheduler.num_timesteps - 1, 0,\n",
    "                                   num_inference_steps, dtype=torch.long)\n",
    "        for t in tqdm(timesteps, desc='Sampling', leave=False):\n",
    "            t_batch = torch.full((b,), t, device=lr.device, dtype=torch.long)\n",
    "            z_t = self.scheduler.ddim_sample(\n",
    "                self.diffusion_unet(z_t, t_batch, context), t, z_t)\n",
    "        return self.decode_latent(z_t)\n",
    "\n",
    "# ---- Quick summary ----\n",
    "_tmp = HNDSR()\n",
    "total_params = sum(p.numel() for p in _tmp.parameters())\n",
    "print(f\"HNDSR total parameters: {total_params:,}\")\n",
    "for name, module in [('Autoencoder', _tmp.autoencoder),\n",
    "                     ('Neural Operator', _tmp.neural_operator),\n",
    "                     ('Implicit Amp', _tmp.implicit_amp),\n",
    "                     ('Diffusion UNet', _tmp.diffusion_unet)]:\n",
    "    n = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"  {name:20s}: {n:>10,}  ({100*n/total_params:.1f}%)\")\n",
    "del _tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca0b10",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Evaluation Metrics\n",
    "\n",
    "We evaluate with three standard super-resolution metrics:\n",
    "\n",
    "| Metric | Full Name | Measures | Better |\n",
    "|--------|-----------|----------|--------|\n",
    "| **PSNR** | Peak Signal-to-Noise Ratio | Pixel-level accuracy | Higher ↑ |\n",
    "| **SSIM** | Structural Similarity Index | Structural preservation | Higher ↑ |\n",
    "| **LPIPS** | Learned Perceptual Similarity | Perceptual quality (deep features) | Lower ↓ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy_01(t):\n",
    "    \"\"\"Convert [-1,1] tensor → [0,1] numpy, shape (B, H, W, C).\"\"\"\n",
    "    return ((t.detach().cpu().numpy().transpose(0, 2, 3, 1)) + 1.0) / 2.0\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    a, b = to_numpy_01(img1), to_numpy_01(img2)\n",
    "    return np.mean([psnr(a[i], b[i], data_range=1.0) for i in range(a.shape[0])])\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    a, b = to_numpy_01(img1), to_numpy_01(img2)\n",
    "    return np.mean([ssim(a[i], b[i], data_range=1.0, channel_axis=2)\n",
    "                    for i in range(a.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d5aec",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training Functions (3-Stage Pipeline)\n",
    "\n",
    "Each stage is trained sequentially; earlier stages are **frozen** before training later ones.\n",
    "\n",
    "| Feature | Details |\n",
    "|---|---|\n",
    "| **Optimizer** | AdamW (decoupled weight decay) |\n",
    "| **LR Schedule** | Linear warmup (`WARMUP_EPOCHS`) → Cosine annealing |\n",
    "| **Gradient Clipping** | `MAX_GRAD_NORM` — prevents exploding gradients in spectral / diffusion layers |\n",
    "| **Early Stopping** | Patience = `EARLY_STOP_PATIENCE` epochs without val-loss improvement |\n",
    "| **EMA** | Exponential Moving Average of diffusion UNet weights (Stage 3 only) |\n",
    "| **Timing** | Per-epoch wall-clock time logged |\n",
    "\n",
    "### Loss Functions\n",
    "- **Stage 1 (AE):** $\\mathcal{L}_{\\text{AE}} = \\|x - \\hat{x}\\|_1$ (L1 for sharper reconstructions)\n",
    "- **Stage 2 (NO):** $\\mathcal{L}_{\\text{NO}} = \\|z_{\\text{pred}} - z_{\\text{target}}\\|_2^2$ (MSE in latent space)\n",
    "- **Stage 3 (Diff):** $\\mathcal{L}_{\\text{Diff}} = \\|\\epsilon - \\epsilon_\\theta(z_t, t, c)\\|_2^2$ (noise-prediction objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "#  Helper: build warmup + cosine scheduler\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "def _make_scheduler(optimizer, warmup_epochs, total_epochs):\n",
    "    \"\"\"Linear warmup for `warmup_epochs`, then cosine annealing to 0.\"\"\"\n",
    "    warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=0.01, total_iters=warmup_epochs)\n",
    "    cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=max(1, total_epochs - warmup_epochs))\n",
    "    return torch.optim.lr_scheduler.SequentialLR(\n",
    "        optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs])\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "#  Stage 1: Autoencoder  (L1 reconstruction)\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "def train_autoencoder(model, train_loader, val_loader, num_epochs, lr=1e-4,\n",
    "                      device='cuda', resume_path=None):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"STAGE 1: Training Autoencoder  ({num_epochs} epochs)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    model.autoencoder.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.autoencoder.parameters(),\n",
    "                                  lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = _make_scheduler(optimizer, WARMUP_EPOCHS, num_epochs)\n",
    "    loss_fn = nn.L1Loss()\n",
    "    best_val = float('inf')\n",
    "    patience_counter = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    history = {'train': [], 'val': [], 'psnr': [], 'lr': []}\n",
    "\n",
    "    # ---- Resume from mid-stage checkpoint ----\n",
    "    if resume_path and os.path.exists(resume_path):\n",
    "        ckpt = torch.load(resume_path, map_location=device, weights_only=False)\n",
    "        model.autoencoder.load_state_dict(ckpt['model_state'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state'])\n",
    "        scheduler.load_state_dict(ckpt['scheduler_state'])\n",
    "        start_epoch    = ckpt['epoch'] + 1\n",
    "        best_val       = ckpt['best_val']\n",
    "        patience_counter = ckpt['patience_counter']\n",
    "        history        = ckpt['history']\n",
    "        print(f\"  ↻ Resumed from epoch {start_epoch}/{num_epochs}  \"\n",
    "              f\"(best_val={best_val:.4f}, patience={patience_counter})\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        t0 = time.time()\n",
    "        model.autoencoder.train()\n",
    "        train_losses = []\n",
    "        for batch in tqdm(train_loader, desc=f\"AE {epoch+1}/{num_epochs}\", leave=False):\n",
    "            hr = batch['hr'].to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            recon, z = model.autoencoder(hr)\n",
    "            loss = loss_fn(recon, hr)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.autoencoder.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            del hr, recon, z, loss\n",
    "\n",
    "        # Validation\n",
    "        model.autoencoder.eval()\n",
    "        val_losses, val_psnrs = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                hr = batch['hr'].to(device)\n",
    "                recon, z = model.autoencoder(hr)\n",
    "                val_losses.append(loss_fn(recon, hr).item())\n",
    "                val_psnrs.append(calculate_psnr(recon, hr))\n",
    "                del hr, recon, z\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        tl, vl, vp = np.mean(train_losses), np.mean(val_losses), np.mean(val_psnrs)\n",
    "        cur_lr = optimizer.param_groups[0]['lr']\n",
    "        history['train'].append(tl); history['val'].append(vl)\n",
    "        history['psnr'].append(vp); history['lr'].append(cur_lr)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Epoch {epoch+1:3d} | Train {tl:.4f} | Val {vl:.4f} | \"\n",
    "              f\"PSNR {vp:.2f} dB | LR {cur_lr:.1e} | {elapsed:.0f}s\", end='')\n",
    "\n",
    "        if vl < best_val:\n",
    "            best_val = vl\n",
    "            patience_counter = 0\n",
    "            torch.save(model.autoencoder.state_dict(), AUTOENCODER_PATH)\n",
    "            print(\"  *saved*\", end='')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        print()\n",
    "        scheduler.step()\n",
    "\n",
    "        # ---- Save resume checkpoint every epoch ----\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.autoencoder.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'scheduler_state': scheduler.state_dict(),\n",
    "            'best_val': best_val,\n",
    "            'patience_counter': patience_counter,\n",
    "            'history': history,\n",
    "        }, STAGE1_RESUME_PATH)\n",
    "\n",
    "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"  Early stopping triggered (patience {EARLY_STOP_PATIENCE})\")\n",
    "            break\n",
    "\n",
    "    # Reload best weights & clean up resume file\n",
    "    model.autoencoder.load_state_dict(torch.load(AUTOENCODER_PATH, map_location=device, weights_only=False))\n",
    "    if os.path.exists(STAGE1_RESUME_PATH):\n",
    "        os.remove(STAGE1_RESUME_PATH)\n",
    "        print(\"  Removed Stage 1 resume checkpoint (training complete)\")\n",
    "    print(\"Stage 1 complete — best val loss: {:.4f}\".format(best_val))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "#  Stage 2: Neural Operator  (MSE in latent space)\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "def train_neural_operator(model, train_loader, val_loader, num_epochs, lr=1e-4,\n",
    "                          device='cuda', resume_path=None):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"STAGE 2: Training Neural Operator  ({num_epochs} epochs)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for p in model.autoencoder.parameters(): p.requires_grad = False\n",
    "    model.autoencoder.eval()\n",
    "    model.neural_operator.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.neural_operator.parameters(),\n",
    "                                  lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = _make_scheduler(optimizer, WARMUP_EPOCHS, num_epochs)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    best_val = float('inf')\n",
    "    patience_counter = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    history = {'train': [], 'val': [], 'lr': []}\n",
    "\n",
    "    # ---- Resume from mid-stage checkpoint ----\n",
    "    if resume_path and os.path.exists(resume_path):\n",
    "        ckpt = torch.load(resume_path, map_location=device, weights_only=False)\n",
    "        model.neural_operator.load_state_dict(ckpt['model_state'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state'])\n",
    "        scheduler.load_state_dict(ckpt['scheduler_state'])\n",
    "        start_epoch    = ckpt['epoch'] + 1\n",
    "        best_val       = ckpt['best_val']\n",
    "        patience_counter = ckpt['patience_counter']\n",
    "        history        = ckpt['history']\n",
    "        print(f\"  ↻ Resumed from epoch {start_epoch}/{num_epochs}  \"\n",
    "              f\"(best_val={best_val:.4f}, patience={patience_counter})\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        t0 = time.time()\n",
    "        model.neural_operator.train()\n",
    "        train_losses = []\n",
    "        for batch in tqdm(train_loader, desc=f\"NO {epoch+1}/{num_epochs}\", leave=False):\n",
    "            lr_img = batch['lr'].to(device)\n",
    "            hr_img = batch['hr'].to(device)\n",
    "            scale  = batch['scale'][0].item()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.no_grad():\n",
    "                target = model.encode_hr(hr_img)\n",
    "            pred = model.get_no_prior(lr_img, scale)\n",
    "            loss = loss_fn(pred, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.neural_operator.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            del lr_img, hr_img, target, pred, loss\n",
    "\n",
    "        model.neural_operator.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                lr_img = batch['lr'].to(device)\n",
    "                hr_img = batch['hr'].to(device)\n",
    "                scale  = batch['scale'][0].item()\n",
    "                target = model.encode_hr(hr_img)\n",
    "                pred = model.get_no_prior(lr_img, scale)\n",
    "                val_losses.append(loss_fn(pred, target).item())\n",
    "                del lr_img, hr_img, target, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        tl, vl = np.mean(train_losses), np.mean(val_losses)\n",
    "        cur_lr = optimizer.param_groups[0]['lr']\n",
    "        history['train'].append(tl); history['val'].append(vl)\n",
    "        history['lr'].append(cur_lr)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Epoch {epoch+1:3d} | Train {tl:.4f} | Val {vl:.4f} | \"\n",
    "              f\"LR {cur_lr:.1e} | {elapsed:.0f}s\", end='')\n",
    "\n",
    "        if vl < best_val:\n",
    "            best_val = vl\n",
    "            patience_counter = 0\n",
    "            torch.save(model.neural_operator.state_dict(), NEURAL_OPERATOR_PATH)\n",
    "            print(\"  *saved*\", end='')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        print()\n",
    "        scheduler.step()\n",
    "\n",
    "        # ---- Save resume checkpoint every epoch ----\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.neural_operator.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'scheduler_state': scheduler.state_dict(),\n",
    "            'best_val': best_val,\n",
    "            'patience_counter': patience_counter,\n",
    "            'history': history,\n",
    "        }, STAGE2_RESUME_PATH)\n",
    "\n",
    "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"  Early stopping triggered (patience {EARLY_STOP_PATIENCE})\")\n",
    "            break\n",
    "\n",
    "    # Reload best weights & clean up resume file\n",
    "    model.neural_operator.load_state_dict(torch.load(NEURAL_OPERATOR_PATH, map_location=device, weights_only=False))\n",
    "    if os.path.exists(STAGE2_RESUME_PATH):\n",
    "        os.remove(STAGE2_RESUME_PATH)\n",
    "        print(\"  Removed Stage 2 resume checkpoint (training complete)\")\n",
    "    print(\"Stage 2 complete — best val loss: {:.4f}\".format(best_val))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68704f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════\n",
    "#  Stage 3: Diffusion Model  (ε-prediction with EMA)\n",
    "# ═══════════════════════════════════════════════════════════\n",
    "def train_diffusion(model, train_loader, val_loader, num_epochs, lr=1e-4,\n",
    "                    device='cuda', ema=None, resume_path=None):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"STAGE 3: Training Diffusion Model  ({num_epochs} epochs)\")\n",
    "    if ema is not None:\n",
    "        print(f\"  EMA enabled (decay={EMA_DECAY})\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for p in model.autoencoder.parameters(): p.requires_grad = False\n",
    "    for p in model.neural_operator.parameters(): p.requires_grad = False\n",
    "    for p in model.implicit_amp.parameters(): p.requires_grad = False\n",
    "    model.autoencoder.eval()\n",
    "    model.neural_operator.eval()\n",
    "    model.implicit_amp.eval()\n",
    "    model.diffusion_unet.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.diffusion_unet.parameters(),\n",
    "                                  lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = _make_scheduler(optimizer, WARMUP_EPOCHS, num_epochs)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    best_val = float('inf')\n",
    "    patience_counter = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    history = {'train': [], 'val': [], 'lr': []}\n",
    "\n",
    "    # ---- Resume from mid-stage checkpoint ----\n",
    "    if resume_path and os.path.exists(resume_path):\n",
    "        ckpt = torch.load(resume_path, map_location=device, weights_only=False)\n",
    "        model.diffusion_unet.load_state_dict(ckpt['model_state'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state'])\n",
    "        scheduler.load_state_dict(ckpt['scheduler_state'])\n",
    "        start_epoch    = ckpt['epoch'] + 1\n",
    "        best_val       = ckpt['best_val']\n",
    "        patience_counter = ckpt['patience_counter']\n",
    "        history        = ckpt['history']\n",
    "        if ema is not None and 'ema_shadow' in ckpt:\n",
    "            ema.shadow = ckpt['ema_shadow']\n",
    "        print(f\"  ↻ Resumed from epoch {start_epoch}/{num_epochs}  \"\n",
    "              f\"(best_val={best_val:.4f}, patience={patience_counter})\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        t0 = time.time()\n",
    "        model.diffusion_unet.train()\n",
    "        train_losses = []\n",
    "        for batch in tqdm(train_loader, desc=f\"Diff {epoch+1}/{num_epochs}\", leave=False):\n",
    "            lr_img = batch['lr'].to(device)\n",
    "            hr_img = batch['hr'].to(device)\n",
    "            scale  = batch['scale'][0].item()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.no_grad():\n",
    "                target = model.encode_hr(hr_img)\n",
    "                no_prior = model.get_no_prior(lr_img, scale)\n",
    "                b = lr_img.shape[0]\n",
    "                context = F.adaptive_avg_pool2d(no_prior, 1).view(b, -1)\n",
    "\n",
    "            timesteps = torch.randint(0, model.scheduler.num_timesteps, (b,), device=device).long()\n",
    "            noise = torch.randn_like(target)\n",
    "            noisy = model.scheduler.add_noise(target, noise, timesteps)\n",
    "            pred  = model.diffusion_unet(noisy, timesteps, context)\n",
    "            loss  = loss_fn(pred, noise)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.diffusion_unet.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "\n",
    "            # EMA update\n",
    "            if ema is not None:\n",
    "                ema.update(model.diffusion_unet)\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            del lr_img, hr_img, target, no_prior, context, timesteps, noise, noisy, pred, loss\n",
    "\n",
    "        model.diffusion_unet.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                lr_img = batch['lr'].to(device)\n",
    "                hr_img = batch['hr'].to(device)\n",
    "                scale  = batch['scale'][0].item()\n",
    "                target = model.encode_hr(hr_img)\n",
    "                no_prior = model.get_no_prior(lr_img, scale)\n",
    "                b = lr_img.shape[0]\n",
    "                context = F.adaptive_avg_pool2d(no_prior, 1).view(b, -1)\n",
    "                timesteps = torch.randint(0, model.scheduler.num_timesteps, (b,), device=device).long()\n",
    "                noise = torch.randn_like(target)\n",
    "                noisy = model.scheduler.add_noise(target, noise, timesteps)\n",
    "                pred  = model.diffusion_unet(noisy, timesteps, context)\n",
    "                val_losses.append(loss_fn(pred, noise).item())\n",
    "                del lr_img, hr_img, target, no_prior, context, timesteps, noise, noisy, pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        tl, vl = np.mean(train_losses), np.mean(val_losses)\n",
    "        cur_lr = optimizer.param_groups[0]['lr']\n",
    "        history['train'].append(tl); history['val'].append(vl)\n",
    "        history['lr'].append(cur_lr)\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"  Epoch {epoch+1:3d} | Train {tl:.4f} | Val {vl:.4f} | \"\n",
    "              f\"LR {cur_lr:.1e} | {elapsed:.0f}s\", end='')\n",
    "\n",
    "        if vl < best_val:\n",
    "            best_val = vl\n",
    "            patience_counter = 0\n",
    "            save_dict = {'diffusion_unet': model.diffusion_unet.state_dict()}\n",
    "            if ema is not None:\n",
    "                save_dict['ema_shadow'] = ema.shadow\n",
    "            torch.save(save_dict, DIFFUSION_PATH)\n",
    "            print(\"  *saved*\", end='')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        print()\n",
    "        scheduler.step()\n",
    "\n",
    "        # ---- Save resume checkpoint every epoch ----\n",
    "        resume_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.diffusion_unet.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'scheduler_state': scheduler.state_dict(),\n",
    "            'best_val': best_val,\n",
    "            'patience_counter': patience_counter,\n",
    "            'history': history,\n",
    "        }\n",
    "        if ema is not None:\n",
    "            resume_dict['ema_shadow'] = ema.shadow\n",
    "        torch.save(resume_dict, STAGE3_RESUME_PATH)\n",
    "\n",
    "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"  Early stopping triggered (patience {EARLY_STOP_PATIENCE})\")\n",
    "            break\n",
    "\n",
    "    # Reload best weights & clean up resume file\n",
    "    ckpt = torch.load(DIFFUSION_PATH, map_location=device, weights_only=False)\n",
    "    model.diffusion_unet.load_state_dict(ckpt['diffusion_unet'])\n",
    "    if ema is not None and 'ema_shadow' in ckpt:\n",
    "        ema.shadow = ckpt['ema_shadow']\n",
    "    if os.path.exists(STAGE3_RESUME_PATH):\n",
    "        os.remove(STAGE3_RESUME_PATH)\n",
    "        print(\"  Removed Stage 3 resume checkpoint (training complete)\")\n",
    "    print(\"Stage 3 complete — best val loss: {:.4f}\".format(best_val))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedcac74",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Checkpoint & Resume Helpers\n",
    "\n",
    "### How resume works\n",
    "Each training function saves a **resume checkpoint every epoch** (`stageN_resume.pth`) containing the model weights, optimizer state, scheduler state, epoch counter, best validation loss, patience counter, and full training history.\n",
    "\n",
    "**If a Kaggle session times out mid-training:**\n",
    "1. Your output files are preserved (they're in `/kaggle/working/`)\n",
    "2. When you start a new session, add your **previous notebook output** as a dataset\n",
    "3. Copy the checkpoint files back: `!cp /kaggle/input/your-output-dataset/* /kaggle/working/`\n",
    "4. **Just re-run all cells** — with `AUTO_RESUME = True` the notebook automatically detects which stage was interrupted and continues from the exact epoch\n",
    "\n",
    "The resume file is deleted once a stage finishes successfully, so it only exists for interrupted stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_stages(model, up_to_stage, device='cuda'):\n",
    "    \"\"\"Load previously saved stage checkpoints up to `up_to_stage` (inclusive).\n",
    "    Returns (model, last_loaded_stage).\n",
    "    \"\"\"\n",
    "    if up_to_stage >= 1 and os.path.exists(AUTOENCODER_PATH):\n",
    "        model.autoencoder.load_state_dict(\n",
    "            torch.load(AUTOENCODER_PATH, map_location=device, weights_only=True))\n",
    "        model.autoencoder.to(device)\n",
    "        print(f\"  ✓ Stage 1 loaded: {AUTOENCODER_PATH}\")\n",
    "    elif up_to_stage >= 1:\n",
    "        print(f\"  ✗ Stage 1 NOT found at {AUTOENCODER_PATH}\")\n",
    "        return model, 0\n",
    "\n",
    "    if up_to_stage >= 2 and os.path.exists(NEURAL_OPERATOR_PATH):\n",
    "        model.neural_operator.load_state_dict(\n",
    "            torch.load(NEURAL_OPERATOR_PATH, map_location=device, weights_only=True))\n",
    "        model.neural_operator.to(device)\n",
    "        print(f\"  ✓ Stage 2 loaded: {NEURAL_OPERATOR_PATH}\")\n",
    "    elif up_to_stage >= 2:\n",
    "        print(f\"  ✗ Stage 2 NOT found at {NEURAL_OPERATOR_PATH}\")\n",
    "        return model, 1\n",
    "\n",
    "    if up_to_stage >= 3 and os.path.exists(DIFFUSION_PATH):\n",
    "        ckpt = torch.load(DIFFUSION_PATH, map_location=device, weights_only=True)\n",
    "        model.diffusion_unet.load_state_dict(ckpt['diffusion_unet'])\n",
    "        model.diffusion_unet.to(device)\n",
    "        print(f\"  ✓ Stage 3 loaded: {DIFFUSION_PATH}\")\n",
    "    elif up_to_stage >= 3:\n",
    "        print(f\"  ✗ Stage 3 NOT found at {DIFFUSION_PATH}\")\n",
    "        return model, 2\n",
    "\n",
    "    return model, up_to_stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f3102",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Run Training Pipeline\n",
    "\n",
    "> **Kaggle tip — multi-session training:**\n",
    "> 1. Run the notebook with **Save & Run All** and let it run overnight\n",
    "> 2. If the 12-hour session expires mid-training, go to the notebook's **Output** tab and click **\"New Notebook\"** or add the output as a dataset input\n",
    "> 3. Copy checkpoint files back to `/kaggle/working/` and **re-run all cells** — training auto-resumes from the last completed epoch\n",
    "> 4. Repeat until all stages are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ee3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Build dataloaders ----\n",
    "full_dataset = SatelliteDataset(HR_DIR, LR_DIR, patch_size=PATCH_SIZE, training=True)\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "val_size   = len(full_dataset) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"Train: {len(train_ds)}  |  Val: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb561ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply EMA weights for final model (smoother, better generalisation)\n",
    "# If `ema` exists from training, apply its shadow weights.\n",
    "# If training already finished (kernel restart / separate cell), skip gracefully.\n",
    "if 'ema' in dir() and ema is not None:\n",
    "    ema.apply(model.diffusion_unet)\n",
    "    print(\"Applied EMA shadow weights to diffusion UNet.\")\n",
    "else:\n",
    "    print(\"EMA not available (training already completed in a prior run). \"\n",
    "          \"Using best-checkpoint weights directly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10370c4",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Training Curves\n",
    "\n",
    "Loss curves for each stage, plus per-stage learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c1f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'all_histories' not in dir() or all_histories is None:\n",
    "    print(\"all_histories not available (training completed in a prior session \"\n",
    "          \"and resume checkpoints — which store history — are cleaned up after each stage).\")\n",
    "    print(\"Skipping training-curve plot.  If you need curves, re-run training \"\n",
    "          \"from Stage 1 in a single session or save histories separately.\")\n",
    "else:\n",
    "    n_plots = len(all_histories)\n",
    "    if n_plots == 0:\n",
    "        print(\"No training histories recorded (did you resume past all stages?).\")\n",
    "    else:\n",
    "        fig, axes = plt.subplots(2, n_plots, figsize=(6*n_plots, 9))\n",
    "        if n_plots == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "\n",
    "        titles = {'stage1': 'Stage 1: Autoencoder (L1)',\n",
    "                  'stage2': 'Stage 2: Neural Operator (MSE)',\n",
    "                  'stage3': 'Stage 3: Diffusion (ε-pred)'}\n",
    "\n",
    "        for col, (key, h) in enumerate(all_histories.items()):\n",
    "            # Row 0: Loss curves\n",
    "            ax = axes[0, col]\n",
    "            ax.plot(h['train'], label='Train', linewidth=1.5)\n",
    "            ax.plot(h['val'],   label='Val',   linewidth=1.5)\n",
    "            ax.set_title(titles.get(key, key), fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
    "            ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "            # Row 1: LR schedule\n",
    "            ax2 = axes[1, col]\n",
    "            if 'lr' in h:\n",
    "                ax2.plot(h['lr'], color='tab:green', linewidth=1.5)\n",
    "                ax2.set_ylabel('Learning Rate')\n",
    "            elif 'psnr' in h:\n",
    "                ax2.plot(h['psnr'], color='tab:orange', linewidth=1.5)\n",
    "                ax2.set_ylabel('PSNR (dB)')\n",
    "            ax2.set_xlabel('Epoch'); ax2.grid(True, alpha=0.3)\n",
    "            ax2.set_title('LR Schedule' if 'lr' in h else 'Validation PSNR')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(SAVE_DIR, 'training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"Saved training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ed5fc",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Evaluation\n",
    "\n",
    "Run super-resolution on held-out patches using 50 DDPM inference steps. Computes **PSNR**, **SSIM**, and optionally **LPIPS** (perceptual loss). Saves LR / SR / HR image triplets for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c72b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, test_loader, device='cuda', save_dir=None, max_vis=10,\n",
    "                   num_inference_steps=50):\n",
    "    \"\"\"Full evaluation with PSNR, SSIM, LPIPS.\"\"\"\n",
    "    if save_dir:\n",
    "        for sub in ['lr', 'sr', 'hr']:\n",
    "            Path(os.path.join(save_dir, sub)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        lpips_fn = lpips.LPIPS(net='alex').to(device)\n",
    "        use_lpips = True\n",
    "        print(\"LPIPS metric enabled (AlexNet backbone)\")\n",
    "    except Exception:\n",
    "        use_lpips = False\n",
    "        print(\"LPIPS not available — reporting PSNR & SSIM only\")\n",
    "\n",
    "    model.autoencoder.eval(); model.neural_operator.eval()\n",
    "    model.implicit_amp.eval(); model.diffusion_unet.eval()\n",
    "\n",
    "    psnr_vals, ssim_vals, lpips_vals = [], [], []\n",
    "\n",
    "    for idx, batch in enumerate(tqdm(test_loader, desc='Evaluating')):\n",
    "        lr_img = batch['lr'].to(device)\n",
    "        hr_img = batch['hr'].to(device)\n",
    "        scale  = batch['scale'][0].item()\n",
    "\n",
    "        sr_img = model.super_resolve(lr_img, scale_factor=scale,\n",
    "                                     num_inference_steps=num_inference_steps)\n",
    "\n",
    "        p = calculate_psnr(sr_img, hr_img)\n",
    "        s = calculate_ssim(sr_img, hr_img)\n",
    "        psnr_vals.append(p); ssim_vals.append(s)\n",
    "        if use_lpips:\n",
    "            lpips_vals.append(lpips_fn(sr_img, hr_img).mean().item())\n",
    "\n",
    "        if save_dir and idx < max_vis:\n",
    "            save_image((lr_img+1)/2, f\"{save_dir}/lr/sample_{idx:03d}.png\")\n",
    "            save_image((sr_img+1)/2, f\"{save_dir}/sr/sample_{idx:03d}.png\")\n",
    "            save_image((hr_img+1)/2, f\"{save_dir}/hr/sample_{idx:03d}.png\")\n",
    "\n",
    "        del lr_img, hr_img, sr_img\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    results = {\n",
    "        'psnr_mean': np.mean(psnr_vals), 'psnr_std': np.std(psnr_vals),\n",
    "        'ssim_mean': np.mean(ssim_vals), 'ssim_std': np.std(ssim_vals),\n",
    "        'psnr_values': psnr_vals, 'ssim_values': ssim_vals,\n",
    "    }\n",
    "    if use_lpips:\n",
    "        results['lpips_mean'] = np.mean(lpips_vals)\n",
    "        results['lpips_std']  = np.std(lpips_vals)\n",
    "        results['lpips_values'] = lpips_vals\n",
    "\n",
    "    print(\"\\n\" + \"=\"*55)\n",
    "    print(\"  EVALUATION RESULTS\")\n",
    "    print(\"=\"*55)\n",
    "    print(f\"  PSNR  : {results['psnr_mean']:.2f} ± {results['psnr_std']:.2f} dB\")\n",
    "    print(f\"  SSIM  : {results['ssim_mean']:.4f} ± {results['ssim_std']:.4f}\")\n",
    "    if use_lpips:\n",
    "        print(f\"  LPIPS : {results['lpips_mean']:.4f} ± {results['lpips_std']:.4f}\")\n",
    "    print(f\"  Samples evaluated: {len(psnr_vals)}\")\n",
    "    print(\"=\"*55)\n",
    "\n",
    "    if save_dir:\n",
    "        torch.save(results, os.path.join(save_dir, 'evaluation_results.pth'))\n",
    "        print(f\"  Saved to {save_dir}/evaluation_results.pth\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Rebuild model from checkpoints if not already in memory ----\n",
    "if 'model' not in dir() or model is None:\n",
    "    print(\"model not in memory — rebuilding from saved checkpoints...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = HNDSR().to(device)\n",
    "    model, loaded_up_to = load_trained_stages(model, up_to_stage=3, device=device)\n",
    "    if loaded_up_to < 3:\n",
    "        raise RuntimeError(f\"Only loaded up to stage {loaded_up_to}; \"\n",
    "                           \"need all 3 stage checkpoints for evaluation.\")\n",
    "    model.eval()\n",
    "    print(f\"Model rebuilt on {device} with all 3 stages.\\n\")\n",
    "\n",
    "# ---- Run evaluation ----\n",
    "test_dataset = SatelliteDataset(HR_DIR, LR_DIR, patch_size=PATCH_SIZE, training=False)\n",
    "num_eval = min(50, len(test_dataset))\n",
    "if len(test_dataset) > num_eval:\n",
    "    indices = np.random.choice(len(test_dataset), num_eval, replace=False)\n",
    "    test_dataset = torch.utils.data.Subset(test_dataset, indices)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "Path(EVAL_RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "results = evaluate_model(model, test_loader, device, save_dir=EVAL_RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb58d2df",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Visual Comparison\n",
    "\n",
    "Side-by-side LR → SR → HR for qualitative assessment. Look for sharpness recovery in building edges, road markings, and vegetation texture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17905627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show side-by-side comparisons\n",
    "sr_files = sorted(Path(EVAL_RESULTS_DIR, 'sr').glob('*.png'))\n",
    "n_show = min(5, len(sr_files))\n",
    "\n",
    "if n_show == 0:\n",
    "    print(\"No evaluation images found — run the evaluation cell first.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(n_show, 3, figsize=(15, 5 * n_show))\n",
    "    if n_show == 1: axes = [axes]\n",
    "\n",
    "    for i in range(n_show):\n",
    "        lr = Image.open(f\"{EVAL_RESULTS_DIR}/lr/sample_{i:03d}.png\")\n",
    "        sr = Image.open(f\"{EVAL_RESULTS_DIR}/sr/sample_{i:03d}.png\")\n",
    "        hr = Image.open(f\"{EVAL_RESULTS_DIR}/hr/sample_{i:03d}.png\")\n",
    "\n",
    "        axes[i][0].imshow(lr); axes[i][0].set_title('LR Input (2 m/px)', fontsize=11)\n",
    "        axes[i][1].imshow(sr); axes[i][1].set_title('HNDSR Output',      fontsize=11)\n",
    "        axes[i][2].imshow(hr); axes[i][2].set_title('HR Ground Truth (0.5 m/px)', fontsize=11)\n",
    "        for ax in axes[i]: ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'visual_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved visual_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSNR & SSIM distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.hist(results['psnr_values'], bins=15, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.axvline(results['psnr_mean'], color='red', linestyle='--', linewidth=2,\n",
    "            label=f\"Mean: {results['psnr_mean']:.2f} dB\")\n",
    "ax1.set_title('PSNR Distribution', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('PSNR (dB)'); ax1.set_ylabel('Count'); ax1.legend()\n",
    "\n",
    "ax2.hist(results['ssim_values'], bins=15, edgecolor='black', alpha=0.7, color='seagreen')\n",
    "ax2.axvline(results['ssim_mean'], color='red', linestyle='--', linewidth=2,\n",
    "            label=f\"Mean: {results['ssim_mean']:.4f}\")\n",
    "ax2.set_title('SSIM Distribution', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('SSIM'); ax2.set_ylabel('Count'); ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, 'metric_distributions.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved metric_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a6f51",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Download Model\n",
    "\n",
    "All files saved to `/kaggle/working/` appear in the **Output** tab of your Kaggle notebook.\n",
    "\n",
    "| File | Description |\n",
    "|---|---|\n",
    "| `hndsr_complete.pth` | Full trained model (all 4 sub-networks) |\n",
    "| `autoencoder_best.pth` | Stage 1 best checkpoint |\n",
    "| `neural_operator_best.pth` | Stage 2 best checkpoint |\n",
    "| `diffusion_best.pth` | Stage 3 best checkpoint (includes EMA shadow) |\n",
    "| `stageN_resume.pth` | Mid-stage resume checkpoint (only if interrupted) |\n",
    "| `evaluation_results/` | Metrics `.pth` + LR/SR/HR images |\n",
    "| `training_curves.png` | Loss & LR schedule plots |\n",
    "| `visual_comparison.png` | Side-by-side SR examples |\n",
    "| `metric_distributions.png` | PSNR & SSIM histograms |\n",
    "\n",
    "### To continue training in a new session\n",
    "1. Save the current notebook output as a **dataset** (Output tab → \"New Dataset\")\n",
    "2. In the new session, add that dataset as input\n",
    "3. Copy files: `!cp /kaggle/input/<dataset-name>/*.pth /kaggle/working/`\n",
    "4. Re-run all cells — the notebook auto-resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all output files with sizes\n",
    "print(\"=\" * 50)\n",
    "print(\"  OUTPUT FILES\")\n",
    "print(\"=\" * 50)\n",
    "total_mb = 0\n",
    "for p in sorted(Path(SAVE_DIR).rglob('*')):\n",
    "    if p.is_file():\n",
    "        size = p.stat().st_size / (1024*1024)\n",
    "        total_mb += size\n",
    "        print(f\"  {p.relative_to(SAVE_DIR):<45s} {size:>7.2f} MB\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  {'Total':<45s} {total_mb:>7.2f} MB\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
